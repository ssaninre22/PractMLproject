---
title: "Are you doing it great?"
author: "Sebastian Sanin"
date: "20/8/2019"
output:
  html_document:
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement – a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. 

In this project, I will use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform 10 repetitions of the barbell lifts correctly and incorrectly in 5 different ways:

  * Class A - Exactly according to the specification.
  * Class B - Throwing the elbows to the front.
  * Class C - Lifting the dumbbell only halfway.
  * Class D - Lowering the dumbbell only halfway.
  * Class E - Throwing the hips to the front.

The exercises were performed by six male participants aged between 20-28 years, with little weight lifting experience. Researchers made sure that all participants could easily simulate the mistakes in a safe and controlled manner by using a relatively light dumbbell (1.25kg).

## Data

In this section I load useful libraries for the analysis. I also load the train and testing datasets provided.

```{r}
suppressWarnings(suppressMessages(library(plyr)))
suppressWarnings(suppressMessages(library(dplyr)))
suppressWarnings(suppressMessages(library(data.table)))
suppressWarnings(suppressMessages(library(ggplot2)))
suppressWarnings(suppressMessages(library(caret)))
suppressWarnings(suppressMessages(library(randomForest)))
train.set0 = read.csv("pml-training.csv",na.strings=c("NA","#DIV/0!",""))
test.set0 = read.csv("pml-testing.csv",na.strings=c("NA","#DIV/0!",""))
```

The **train.set** dataframe contain all the variables needed for the training of the model, while the **testing.set** contain the data to conduct the prediction exercises with the trained and validated model. The goal of the project is to predict the "classe" variable.

## Exploratory analysis and cleaning

In order to conduct an accurate prediction exercise I first clean the training and testing dataset. Columns 1 to 7 are informative but useless for the prediction task then I drop them. I also drop columns with NAs.

```{r}
## Set seed
set.seed(1208)
## Get rid of NAs observations and columns that do not contain info for prediction.
# Train set
train.set <- train.set0[,-c(1:7)]
train.set <- train.set[,(colSums(is.na(train.set))==0)]
# Test set
test.set <- test.set0[,-c(1:7)]
test.set <- test.set[,(colSums(is.na(test.set))==0)]
```
Both train.set and test.set consist of about 52 predictors and the classe variable.

## Model set-up

The first step is to create a data partition from the training set. I will create the **training** set that contains a 70\% observations chosen by random samplimg the cleaned test.set. The **validation** set contains the remaining observations in order to validate the model and compute an estimated prediction error.

```{r}
# Training and testing sets
inTrain = createDataPartition(train.set$classe, p = 0.7,list=F)
training = train.set[inTrain,]
validation = train.set[-inTrain,]
```

Given that this is a classification task (classe is a categorical variable), and because its reputation as one of the best predictors for classification, I choose the *random forest* methodology. I change some control parameters in order to use *repeated cross-validation* with 5 folds and 10 repeats. The number of trees taken into account in the forest will be set to 50 in order to look for higher accuracy but without spending too much time in the computation.

```{r}
# Control parameters
c.param <- trainControl(method="repeatedcv",number = 5,repeats = 10)

## MODELLING
rf.model <- train(classe ~ ., data=training, method="rf",
                 trControl=c.param, ntree=50)
rf.model
```

It can be seen that using both, **Accuracy** and **Kappa** criteria, the tuning parameter *mtry* is set to 27. This parameter refers to the number of variables available for splitting at each tree node. Good classification is done for almost 98.8\% of the data in an in-sample basis. The following plot also shows the 10 main variables in the random forest. They are organized respectly the mean decrease in Gini index. **roll_belt** seems to be the most important variable to predict.

```{r}
varImpPlot(rf.model$finalModel,n.var = 10,sort = T,main="The 10-most important variables")
```

## Out-of-sample prediction error

The validation of the model is done using the **validation** set built above. I compute the confusion matrix statistics, and a heatmap plot of it.

```{r}
## Prediction and accuracy testing
rf.predict <- predict(rf.model, validation)
pred.eval.result <- confusionMatrix(validation$classe, rf.predict)
# Overall accuracy measures
pred.eval.result$overall
# Confusion matrix
conf.mat <- as.data.frame(pred.eval.result$table)
conf.mat$Freq <- factor(conf.mat$Freq)

colors <- colorRampPalette(c("lightblue","darkblue"))(15)
ggplot(conf.mat,aes(x=Reference,y=Prediction,fill=Freq))+
  geom_tile()+labs(title = "Confusion matrix heatmap")+
  scale_fill_manual(values=colors,breaks=levels(conf.mat$Freq)[seq(1, nlevels(conf.mat$Freq), by=2)])
```

The figure shows that most of the out-of-sample predictions for extreme categories (A and E) are perfect classified. The other categories have relatively small bad classifications (2 to 4 wrong-classified observations each).

Finally, the computation of the estimated prediction error is done below. It is computed using the **Acurracy** and **Kappa** measures mainly because **Kappa** is a best indicator for categorical variables.

```{r}
est.pred.error.acc <- 1-pred.eval.result$overall[1]
est.pred.error.kappa <- 1-pred.eval.result$overall[2]
print(rbind(c("Accuracy","Kappa"),c(paste(round(est.pred.error.acc*100,3),"%",sep=""),paste(round(est.pred.error.kappa*100,3),"%",sep=""))))
```

It is shown that the prediction error is less than 1\%, really small.

## Prediction exercise

Finally, I conduct the prediction exercise using the **test.set** in order to classify some observations into the above categories.

```{r}
## PREDICTION
results <- predict(rf.model, 
                   test.set[, -length(names(test.set))])
results
```

According to the coursera quiz all of my results are perfect classified, showing the accuracy power of my random forest approach.

## References

Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human ’13). Stuttgart, Germany: ACM SIGCHI, 2013.

